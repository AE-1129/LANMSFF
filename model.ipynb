{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-c4NhOPj_rc"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Lambda, ReLU, Add,Dropout, Activation, Flatten, Input, PReLU,SeparableConv2D, Conv2DTranspose,concatenate,Convolution2D,ZeroPadding2D,Add,MaxPool2D\n",
        "from tensorflow.keras.layers import Conv2D,Conv2DTranspose, Activation,MaxPooling3D, MaxPooling2D, BatchNormalization, UpSampling2D,AveragePooling2D,GlobalMaxPooling2D,GlobalAveragePooling2D\n",
        "import random\n",
        "\n",
        "\n",
        "class PWFS(Layer):\n",
        "    def __init__(self):\n",
        "        super(PWFS, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # Split feature map to 3 sub-group channel-wisely\n",
        "        split1, split2, split3 = tf.split(inputs, num_or_size_splits=3, axis=-1)\n",
        "\n",
        "        # Compute median using element-wise operations and minimum/maximum functions\n",
        "        min_split = tf.minimum(tf.minimum(split1, split2), split3)\n",
        "        max_split = tf.maximum(tf.maximum(split1, split2), split3)\n",
        "        median_values = split1 + split2 + split3 - min_split - max_split\n",
        "\n",
        "        # Averaging max and median sub-group\n",
        "        average_values = 0.5 * (max_split + median_values)\n",
        "\n",
        "\n",
        "        return average_values\n",
        "\n",
        "    def get_config(self):\n",
        "        # No additional hyperparameters to configure\n",
        "        config = super(MFM, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def MassAtt(input_tensor, ratio=4):\n",
        "\n",
        "        # Channel Attention Map\n",
        "        num_input_channels = input_tensor.get_shape().as_list()[-1]\n",
        "        # Squeeze operation: Global average pooling\n",
        "        squeeze = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)\n",
        "        # Excitation operation: Two fully connected layers\n",
        "        excitation = tf.keras.layers.Dense(units=num_input_channels // ratio, activation='relu')(squeeze)\n",
        "        channel_att_map = tf.keras.layers.Dense(units=num_input_channels, activation='sigmoid')(excitation)\n",
        "\n",
        "\n",
        "        # Spatial Attention Map\n",
        "        spatial_attention = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_tensor)\n",
        "        x= tf.keras.layers.Conv2D(filters= 2,kernel_size=3,kernel_initializer='he_uniform',activation='relu',strides=2,padding='same')(spatial_attention)\n",
        "        x= tf.keras.layers.Conv2D(filters= 4,kernel_size=3,kernel_initializer='he_uniform',activation='relu',strides=2,padding='same')(x)\n",
        "        x = Conv2DTranspose(4, (3, 3), activation='relu', padding='same',strides=2)(x)\n",
        "        spatial_att_map = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same',strides=2)(x)\n",
        "\n",
        "        # attention\n",
        "        attention= channel_att_map * spatial_att_map * input_tensor\n",
        "\n",
        "        return attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def two_path(input_tensor, filters, kernel_size, strides=(1, 1), padding='valid'):\n",
        "\n",
        "        # Get input shape\n",
        "        input_shape = input_tensor.get_shape().as_list()\n",
        "        batch_size, height, width, input_channels = input_shape\n",
        "\n",
        "        # Calculate the number of channels per group\n",
        "        channels_per_group = input_channels // 2\n",
        "        filters_per_group = filters // 2\n",
        "\n",
        "\n",
        "\n",
        "        # Shuffle the channel indices randomly\n",
        "        channel_indices = list(range(input_channels))\n",
        "        random.shuffle(channel_indices)\n",
        "\n",
        "        # Rearrange the input tensor based on shuffled channel indices\n",
        "        input_tensor_shuffled = tf.gather(input_tensor, channel_indices, axis=-1)\n",
        "\n",
        "\n",
        "        # Split input and filters into groups\n",
        "        input_groups = tf.split(input_tensor_shuffled, 2, axis=-1)\n",
        "\n",
        "\n",
        "        # H path- First stage of convolution\n",
        "        convH1 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(input_groups[0])\n",
        "        convH1= BatchNormalization()(convH1)\n",
        "        convH1= ReLU()(convH1)\n",
        "\n",
        "\n",
        "\n",
        "        # L path- First stage of convolution\n",
        "        convL1 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(input_groups[1])\n",
        "        convL1    = BatchNormalization()(convL1)\n",
        "        convL1    = ReLU()(convL1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concat first stage\n",
        "        X1     = tf.concat([convH1,convL1], axis=-1)\n",
        "\n",
        "\n",
        "        # H path- Second stage of convolution\n",
        "        convH2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X1)\n",
        "        convH2= BatchNormalization()(convH2)\n",
        "        convH2= ReLU()(convH2)\n",
        "\n",
        "\n",
        "\n",
        "        # L path- Second stage of convolution\n",
        "        convL2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X1)\n",
        "        convL2= BatchNormalization()(convL2)\n",
        "        convL2= ReLU()(convL2)\n",
        "\n",
        "\n",
        "        # Concat second stage\n",
        "        X2     = tf.concat([convH2,convL2], axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "        # H-path-Third stage of convolution\n",
        "        convH3 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X2)\n",
        "        convH3= BatchNormalization()(convH3)\n",
        "        convH3= ReLU()(convH3)\n",
        "\n",
        "\n",
        "        # L-path-Third stage of convolution\n",
        "        convL3 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X2)\n",
        "        convL3= BatchNormalization()(convL3)\n",
        "        convL3= ReLU()(convL3)\n",
        "\n",
        "\n",
        "        # Final concat\n",
        "        output_tensor = tf.concat([convH3,convL3], axis=-1)\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "#********************************\n",
        "#********************************\n",
        "\n",
        "\n",
        "\n",
        "input = tensorflow.keras.Input(shape=(64, 64, 1))\n",
        "\n",
        "# Block 1\n",
        "\n",
        "b1= tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(input)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = tf.keras.layers.SeparableConv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b1)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b1)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = MaxPooling2D(pool_size=2)(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = Dropout(0.4)(b1)\n",
        "\n",
        "# Block 2\n",
        "\n",
        "b2 = two_path(b1, filters=72, kernel_size=3, strides=(1, 1), padding='same')\n",
        "b2 = MassAtt(b2, ratio=4)\n",
        "b2 = Conv2D(72, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b2)\n",
        "b2 = BatchNormalization()(b2)\n",
        "b2 = MaxPooling2D(pool_size=2)(b2)\n",
        "b2 = ReLU()(b2)\n",
        "b2 = Dropout(0.4)(b2)\n",
        "\n",
        "# Block 3\n",
        "\n",
        "b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b2)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = ReLU()(b3)\n",
        "b3 = tf.keras.layers.SeparableConv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b3)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = ReLU()(b3)\n",
        "b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b3)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = MaxPooling2D(pool_size=2)(b3)\n",
        "b3= ReLU()(b3)\n",
        "b3= Dropout(0.4)(b3)\n",
        "\n",
        "# Block 4\n",
        "\n",
        "b4 = two_path(b3, filters=84, kernel_size=3, strides=(1, 1), padding='same')\n",
        "b4 = MassAtt(b4, ratio=4)\n",
        "b4 = Conv2D(84, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b4)\n",
        "b4 = BatchNormalization()(b4)\n",
        "b4 = MaxPooling2D(pool_size=2)(b4)\n",
        "b4 = ReLU()(b4)\n",
        "b4 = Dropout(0.4)(b4)\n",
        "\n",
        "b1 = PWFS()(b1)\n",
        "b2 = PWFS()(b2)\n",
        "b3 = PWFS()(b3)\n",
        "\n",
        "b1 = GlobalAveragePooling2D()(b1)\n",
        "b2 = GlobalAveragePooling2D()(b2)\n",
        "b3 = GlobalAveragePooling2D()(b3)\n",
        "b4 = GlobalAveragePooling2D()(b4)\n",
        "\n",
        "f = tf.concat([b1,b2,b3,b4], axis=-1)\n",
        "\n",
        "output= Dense(7, activation='softmax')(f)\n",
        "\n",
        "model = tensorflow.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "                optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "learning_rate_reducer = ReduceLROnPlateau('val_loss', factor=0.1, patience=9, verbose=1, mode='auto')\n",
        "tensorboard = TensorBoard(log_dir='./logs')\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=14, verbose=1, mode='auto')\n",
        "\n",
        "# Fit the model on the current fold\n",
        "model.fit(X_train, Y_train,\n",
        "            batch_size=32,\n",
        "            epochs=100,\n",
        "            verbose=1,\n",
        "            validation_data=(x_val_fold, y_val_fold),\n",
        "            callbacks=[learning_rate_reducer, tensorboard, early_stopper])\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate(np.array(x_val_fold), np.array(y_val_fold), batch_size=32)\n",
        "print('test_pub_acc:', test_acc, 'test_loss', test_loss)\n",
        "\n"
      ]
    }
  ]
}