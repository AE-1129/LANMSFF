{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Lambda, ReLU, Add,Dropout, Activation, Flatten, Input, PReLU,SeparableConv2D, Conv2DTranspose,concatenate,Convolution2D,ZeroPadding2D,Add,MaxPool2D\n",
        "from tensorflow.keras.layers import Conv2D,Conv2DTranspose, Activation,MaxPooling3D, MaxPooling2D, BatchNormalization, UpSampling2D,AveragePooling2D,GlobalMaxPooling2D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2,l1\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,LearningRateScheduler, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "from tensorflow.keras.layers import Layer, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2D, Multiply, Concatenate, Add\n",
        "import keras.ops as ops\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ChannelShuffle(Layer):\n",
        "    def __init__(self, groups=2, **kwargs):\n",
        "        super(ChannelShuffle, self).__init__(**kwargs)\n",
        "        self.groups = groups\n",
        "\n",
        "    def call(self, x):\n",
        "        # x: (batch, h, w, c)\n",
        "        batch_size, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
        "        channels_per_group = c // self.groups\n",
        "        x = tf.reshape(x, [batch_size, h, w, self.groups, channels_per_group])\n",
        "        x = tf.transpose(x, [0, 1, 2, 4, 3])\n",
        "        x = tf.reshape(x, [batch_size, h, w, c])\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def two_path(input_tensor, filters, kernel_size, strides=(1, 1), padding='valid'):\n",
        "\n",
        "\n",
        "\n",
        "        filters_per_group = filters // 2\n",
        "\n",
        "        input_tensor_shuffled = ChannelShuffle(groups=2)(input_tensor)\n",
        "\n",
        "        group1, group2 = ops.split(input_tensor_shuffled, 2, axis=-1)\n",
        "\n",
        "\n",
        "        # H path- First stage of convolution\n",
        "        convH1 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(group1)\n",
        "        convH1= BatchNormalization()(convH1)\n",
        "        convH1= ReLU()(convH1)\n",
        "\n",
        "\n",
        "\n",
        "        # L path- First stage of convolution\n",
        "        convL1 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(group2)\n",
        "        convL1    = BatchNormalization()(convL1)\n",
        "        convL1    = ReLU()(convL1)\n",
        "\n",
        "\n",
        "\n",
        "        # Concat first stage\n",
        "        X1     = Concatenate(axis=-1)([convH1,convL1])\n",
        "\n",
        "\n",
        "        # H path- Second stage of convolution\n",
        "        convH2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          depthwise_initializer='he_uniform',\n",
        "                                          pointwise_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X1)\n",
        "        convH2= BatchNormalization()(convH2)\n",
        "        convH2= ReLU()(convH2)\n",
        "\n",
        "\n",
        "\n",
        "        # L path- Second stage of convolution\n",
        "        convL2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          depthwise_initializer='he_uniform',\n",
        "                                          pointwise_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X1)\n",
        "        convL2= BatchNormalization()(convL2)\n",
        "        convL2= ReLU()(convL2)\n",
        "\n",
        "\n",
        "        # Concat second stage\n",
        "        X2     = Concatenate(axis=-1)([convH2,convL2])\n",
        "\n",
        "\n",
        "\n",
        "        # H-path-Third stage of convolution\n",
        "        convH3 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X2)\n",
        "        convH3= BatchNormalization()(convH3)\n",
        "        convH3= ReLU()(convH3)\n",
        "\n",
        "\n",
        "        # L-path-Third stage of convolution\n",
        "        convL3 = tf.keras.layers.Conv2D(filters=filters_per_group,\n",
        "                                          kernel_size=kernel_size,\n",
        "                                          kernel_initializer='he_uniform',\n",
        "                                          dilation_rate= 2,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding)(X2)\n",
        "        convL3= BatchNormalization()(convL3)\n",
        "        convL3= ReLU()(convL3)\n",
        "\n",
        "\n",
        "        # Final concat\n",
        "        output_tensor = Concatenate(axis=-1)([convH3,convL3])\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "#********************************\n",
        "#********************************\n",
        "\n",
        "\n",
        "\n",
        "input = tf.keras.Input(shape=(64, 64, 1))\n",
        "\n",
        "# Block 1\n",
        "\n",
        "b1= tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(input)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = tf.keras.layers.SeparableConv2D(filters=66, kernel_size=(3, 3), depthwise_initializer='he_uniform', pointwise_initializer='he_uniform', padding='same')(b1)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b1)\n",
        "b1 = BatchNormalization()(b1)\n",
        "b1 = MaxPooling2D(pool_size=2)(b1)\n",
        "b1 = ReLU()(b1)\n",
        "b1 = Dropout(0.4)(b1)\n",
        "\n",
        "# Block 2\n",
        "\n",
        "b2i = two_path(b1, filters=72, kernel_size=3, strides=(1, 1), padding='same')\n",
        "b2 = MassAtt(b2i, ratio=4)\n",
        "b2 = b2 * b2i\n",
        "b2 = Conv2D(72, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b2)\n",
        "b2 = BatchNormalization()(b2)\n",
        "b2 = MaxPooling2D(pool_size=2)(b2)\n",
        "b2 = ReLU()(b2)\n",
        "b2 = Dropout(0.4)(b2)\n",
        "\n",
        "# Block 3\n",
        "\n",
        "b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b2)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = ReLU()(b3)\n",
        "b3 = tf.keras.layers.SeparableConv2D(filters=78, kernel_size=(3, 3), depthwise_initializer='he_uniform', pointwise_initializer='he_uniform', padding='same')(b3)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = ReLU()(b3)\n",
        "b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b3)\n",
        "b3 = BatchNormalization()(b3)\n",
        "b3 = MaxPooling2D(pool_size=2)(b3)\n",
        "b3= ReLU()(b3)\n",
        "b3= Dropout(0.4)(b3)\n",
        "\n",
        "# Block 4\n",
        "\n",
        "b4i = two_path(b3, filters=84, kernel_size=3, strides=(1, 1), padding='same')\n",
        "b4 = MassAtt(b4i, ratio=4)\n",
        "b4 = b4 * b4i\n",
        "b4 = Conv2D(84, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b4)\n",
        "b4 = BatchNormalization()(b4)\n",
        "b4 = MaxPooling2D(pool_size=2)(b4)\n",
        "b4 = ReLU()(b4)\n",
        "b4 = Dropout(0.4)(b4)\n",
        "\n",
        "b1 = PWFS()(b1)\n",
        "b2 = PWFS()(b2)\n",
        "b3 = PWFS()(b3)\n",
        "\n",
        "b1 = GlobalAveragePooling2D()(b1)\n",
        "b2 = GlobalAveragePooling2D()(b2)\n",
        "b3 = GlobalAveragePooling2D()(b3)\n",
        "b4 = GlobalAveragePooling2D()(b4)\n",
        "\n",
        "f = Concatenate(axis=-1)([b1,b2,b3,b4])\n",
        "\n",
        "output= Dense(8, activation='softmax')(f)\n",
        "\n",
        "model = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=CategoricalCrossentropy,\n",
        "                optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "learning_rate_reducer = ReduceLROnPlateau('val_loss', factor=0.1, patience=9, verbose=1, mode='auto')\n",
        "tensorboard = TensorBoard(log_dir='./logs')\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=14, verbose=1, mode='auto')\n",
        "\n",
        "# Fit the model on the current fold\n",
        "model.fit(X_train, Y_train,\n",
        "            batch_size=32,\n",
        "            epochs=100,\n",
        "            verbose=1,\n",
        "            validation_data=(x_val_fold, y_val_fold),\n",
        "            callbacks=[learning_rate_reducer, tensorboard, early_stopper])\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate(np.array(x_val_fold), np.array(y_val_fold), batch_size=32)\n",
        "print('test_pub_acc:', test_acc, 'test_loss', test_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "KMkGVKM7aui6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}