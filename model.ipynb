def two_path(input_tensor, filters, kernel_size, strides=(1, 1), padding='valid'):



        filters_per_group = filters // 2

        input_tensor_shuffled = ChannelShuffle(groups=2)(input_tensor)

        group1, group2 = ops.split(input_tensor_shuffled, 2, axis=-1)


        # H path- First stage of convolution
        convH1 = tf.keras.layers.Conv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          kernel_initializer='he_uniform',
                                          strides=strides,
                                          padding=padding)(group1)
        convH1= BatchNormalization()(convH1)
        convH1= ReLU()(convH1)



        # L path- First stage of convolution
        convL1 = tf.keras.layers.Conv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          kernel_initializer='he_uniform',
                                          dilation_rate= 2,
                                          strides=strides,
                                          padding=padding)(group2)
        convL1    = BatchNormalization()(convL1)
        convL1    = ReLU()(convL1)



        # Concat first stage
        X1     = Concatenate(axis=-1)([convH1,convL1])


        # H path- Second stage of convolution
        convH2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          depthwise_initializer='he_uniform',
                                          pointwise_initializer='he_uniform',
                                          strides=strides,
                                          padding=padding)(X1)
        convH2= BatchNormalization()(convH2)
        convH2= ReLU()(convH2)



        # L path- Second stage of convolution
        convL2 = tf.keras.layers.SeparableConv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          depthwise_initializer='he_uniform',
                                          pointwise_initializer='he_uniform',
                                          dilation_rate= 2,
                                          strides=strides,
                                          padding=padding)(X1)
        convL2= BatchNormalization()(convL2)
        convL2= ReLU()(convL2)


        # Concat second stage
        X2     = Concatenate(axis=-1)([convH2,convL2])



        # H-path-Third stage of convolution
        convH3 = tf.keras.layers.Conv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          kernel_initializer='he_uniform',
                                          strides=strides,
                                          padding=padding)(X2)
        convH3= BatchNormalization()(convH3)
        convH3= ReLU()(convH3)


        # L-path-Third stage of convolution
        convL3 = tf.keras.layers.Conv2D(filters=filters_per_group,
                                          kernel_size=kernel_size,
                                          kernel_initializer='he_uniform',
                                          dilation_rate= 2,
                                          strides=strides,
                                          padding=padding)(X2)
        convL3= BatchNormalization()(convL3)
        convL3= ReLU()(convL3)


        # Final concat
        output_tensor = Concatenate(axis=-1)([convH3,convL3])

        return output_tensor

#********************************
#********************************



input = tf.keras.Input(shape=(64, 64, 1))

# Block 1

b1= tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(input)
b1 = BatchNormalization()(b1)
b1 = ReLU()(b1)
b1 = tf.keras.layers.SeparableConv2D(filters=66, kernel_size=(3, 3), depthwise_initializer='he_uniform', pointwise_initializer='he_uniform', padding='same')(b1)
b1 = BatchNormalization()(b1)
b1 = ReLU()(b1)
b1 = tf.keras.layers.Conv2D(filters=66, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b1)
b1 = BatchNormalization()(b1)
b1 = MaxPooling2D(pool_size=2)(b1)
b1 = ReLU()(b1)
b1 = Dropout(0.4)(b1)

# Block 2

b2 = two_path(b1, filters=72, kernel_size=3, strides=(1, 1), padding='same')
b2 = MassAtt(b2, ratio=4)
b2 = Conv2D(72, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b2)
b2 = BatchNormalization()(b2)
b2 = MaxPooling2D(pool_size=2)(b2)
b2 = ReLU()(b2)
b2 = Dropout(0.4)(b2)

# Block 3

b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b2)
b3 = BatchNormalization()(b3)
b3 = ReLU()(b3)
b3 = tf.keras.layers.SeparableConv2D(filters=78, kernel_size=(3, 3), depthwise_initializer='he_uniform', pointwise_initializer='he_uniform', padding='same')(b3)
b3 = BatchNormalization()(b3)
b3 = ReLU()(b3)
b3 = tf.keras.layers.Conv2D(filters=78, kernel_size=(3, 3), kernel_initializer='he_uniform', padding='same')(b3)
b3 = BatchNormalization()(b3)
b3 = MaxPooling2D(pool_size=2)(b3)
b3= ReLU()(b3)
b3= Dropout(0.4)(b3)

# Block 4

b4 = two_path(b3, filters=84, kernel_size=3, strides=(1, 1), padding='same')
b4 = MassAtt(b4, ratio=4)
b4 = Conv2D(84, kernel_size=(1, 1),kernel_initializer='he_uniform', padding='same')(b4)
b4 = BatchNormalization()(b4)
b4 = MaxPooling2D(pool_size=2)(b4)
b4 = ReLU()(b4)
b4 = Dropout(0.4)(b4)

b1 = PWFS()(b1)
b2 = PWFS()(b2)
b3 = PWFS()(b3)

b1 = GlobalAveragePooling2D()(b1)
b2 = GlobalAveragePooling2D()(b2)
b3 = GlobalAveragePooling2D()(b3)
b4 = GlobalAveragePooling2D()(b4)

f = Concatenate(axis=-1)([b1,b2,b3,b4])

output= Dense(8, activation='softmax')(f)

model = tf.keras.Model(inputs=input, outputs=output)

model.summary()
