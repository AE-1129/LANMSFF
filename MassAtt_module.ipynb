{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3stgXmt5dNI"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Lambda, ReLU, Add,Dropout, Activation, Flatten, Input, PReLU,SeparableConv2D, Conv2DTranspose,concatenate,Convolution2D,ZeroPadding2D,Add,MaxPool2D\n",
        "from tensorflow.keras.layers import Conv2D,Conv2DTranspose, Activation,MaxPooling3D, MaxPooling2D, BatchNormalization, UpSampling2D,AveragePooling2D,GlobalMaxPooling2D,GlobalAveragePooling2D\n",
        "import random\n",
        "\n",
        "\n",
        "def MassAtt(input_tensor, ratio=4):\n",
        "\n",
        "        # Channel Attention Map\n",
        "        num_input_channels = input_tensor.get_shape().as_list()[-1]\n",
        "        # Squeeze operation: Global average pooling\n",
        "        squeeze = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)\n",
        "        # Excitation operation: Two fully connected layers\n",
        "        excitation = tf.keras.layers.Dense(units=num_input_channels // ratio, activation='relu')(squeeze)\n",
        "        channel_att_map = tf.keras.layers.Dense(units=num_input_channels, activation='sigmoid')(excitation)\n",
        "\n",
        "\n",
        "        # Spatial Attention Map\n",
        "        spatial_attention = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(input_tensor)\n",
        "        x= tf.keras.layers.Conv2D(filters= 2,kernel_size=3,kernel_initializer='he_uniform',activation='relu',strides=2,padding='same')(spatial_attention)\n",
        "        x= tf.keras.layers.Conv2D(filters= 4,kernel_size=3,kernel_initializer='he_uniform',activation='relu',strides=2,padding='same')(x)\n",
        "        x = Conv2DTranspose(4, (3, 3), activation='relu', padding='same',strides=2)(x)\n",
        "        spatial_att_map = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same',strides=2)(x)\n",
        "\n",
        "        # attention\n",
        "        attention= channel_att_map * spatial_att_map * input_tensor\n",
        "\n",
        "        return attention"
      ]
    }
  ]
}